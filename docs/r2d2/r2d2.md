# Single Machine R2D2

## Overview

Recurrent Replay Distributed DQN (R2D2) is a variant of the classic off-policy, value-based DQN algorithm with two key features:

1. a recurrent neural network (RNN) for the Q-function
2. a distributed prioritized experience replay buffer, which 

The RNN is trained on sequences of observations, rather than just the usual current observation for most popular DQN agents. The distributed replay buffer is designed to support multiple actors collecting experience in parallel, which can then used by a central learner for updating the Q-function. This combination of RNN based policy plus distributed training allowed R2D2 to achieve SOTA results in Atari and DM-Lab in less wall clock time.

This implementation of R2D2 is designed to be used with a single machine with multiple CPU cores and a single GPU (although GPU is not required, but recommended). Restricting the implementation to a single machine means we can use a relatively simple distributed design and take advantage of using shared memory for communication between processes. It does however limit the scalability of the implementation to the number of cores available on a single machine.

Original Paper:

- [Recurrent Experience Replay in Distributed Reinforcement Learning](https://openreview.net/pdf?id=r1lyTjAqYX)

Further reading and resources:

- [Ape-X paper: Distributed Prioritized Experience Replay](https://arxiv.org/abs/1803.00933) which introduced the distributed replay buffer extended by R2D2
- [Priotizied Experience Replay Paper](https://arxiv.org/pdf/1511.05952.pdf)
- [Deep RL Zoo](https://github.com/michaelnny/deep_rl_zoo) which has a PyTorch implementation of R2D2

## Usage

The core R2D2 algorithm is implemented in [r2d2.py](https://github.com/Jjschwartz/miniDRL/blob/main/minidrl/r2d2/r2d2.py) and [replay.py](https://github.com/Jjschwartz/miniDRL/blob/main/minidrl/r2d2/replay.py). It cannot be run directly, but contains the full implementation of R2D2 across the two files. The only thing not included is environment specific code (e.g. environment and network creation functions). It is used by the following scripts, which contain the environment specific code:

### ``r2d2/run_gym.py``

This script can be used to train a R2D2 agent on any [Gymnasium](https://gymnasium.farama.org/) environment that has discrete actions. For example, run:

```bash
# to get all options
python minidrl/r2d2/run_gym.py --help
# to train an agent on the `CartPole-v1` environment
python minidrl/r2d2/run_gym.py --env_id CartPole-v1
```

### ``r2d2/run_atari.py``

This script can be used to train a R2D2 agent on [Atari](https://gymnasium.farama.org/environments/atari/) environments. For example, run:

```bash
# to get all options
python minidrl/r2d2/run_atari.py --help
# to train an agent on the `PongNoFrameskip-v4` environment
python minidrl/r2d2/run_atari.py --env_id PongNoFrameskip-v4
```

> **_NOTE:__** Running R2D2 in Atari with the default hyperparamters (the ones used in the paper) requires a lot of RAM for the replay buffer. If you run out of memory, try reducing the replay buffer size by setting `--replay_buffer_size` to a smaller value (e.g. `--replay_buffer_size 10000`) or reduce the sequence lengths (e.g. `--seq_len 10`, `--burin_len 0`). Depending on the environment this may reduce performance.

## Explanation of logged metrics

Running the miniDRL R2D2 implementaion will automatically record various metrics such as losses and mean episode returns in [Tensorboard](https://www.tensorflow.org/tensorboard) (and optionally [WandB](https://wandb.ai/)). Below is some information for these metrics:

- `charts/global_step`: the current training step. This is the total number of individual environment steps (i.e. frames) used to train the model.
- `charts/update`: the number of policy network updates performed
- `charts/learner_SPS`: number of environment steps per second processed for updating the network 
- `charts/learner_UPS`: number of policy network updates per second
- `charts/sample_time`: the time required to sample a batch of data from the replay buffer each update
- `charts/burnin_time`: the time required for performing the sequence burnin (i.e. getting initial lstm state) each update
- `charts/learning_time`: the time required for computing the loss and backpropogation each update
- `charts/update_time`: the total time required each update
- `losses/learning_rate`: the current learning rate
- `losses/value_loss`: the mean value loss across batch
- `losses/q_max`: the max Q-value for any sequence in the batch
- `losses/[mean,min,max]_priorities`: the mean/min/max priority assigned to the sequences in the batch
- `losses/unclipped_grad_norm`: the norm of gradients before any gradient clipping
- `actor/actor_steps`: the total number of environment steps processed on an individual actor
- `actor/actor_sps`: the number of environment steps per second processed on an individual actor
- `actor/estimated_total_actor_sps`: the estimated total number of environment steps per second processed across all actors (this is an estimate based on the number of actors and the mean actor SPS)
- `actor/[mean,min,max]_episode_return`: the mean/min/max episode return generated on the actor
- `actor/mean_episode_lengths`: the mean episode length generated on the actor
- `actor/actor_episodes_completed`: the total number of episodes completed by an individual actor
- `replay/size`: the number of sequences currently in the replay buffer
- `replay/seqs_added`: the total number of sequences that have been added to the replay buffer
- `replay/steps_added`: the total number of individual environment steps (i.e. frames) that have been added to the replay buffer (`seqs_added * seq_len`)
- `replay/seqs_sampled`: the total number of sequences that have been sampled from the replay buffer by the learner
- `replay/replay_ratio`: the "replay ratio", which is the effective number of time each experienced sequence has been used by the learner for training the policy network. This is computed as `seqs_sampled / seqs_added`.
- `replay/added_seq_per_sec`: the number of sequences added to the replay buffer per second by the actors
- `replay/sampled_seq_per_sec`: the number of sequences sampled from the replay buffer per second by the learner
- `replay/q_size`: the number of sequences currently in the replay buffer queue waiting to be added to the replay (default max is 100)


> **_NOTE:__** The `actor` logged metrics are produced by a single actor. Specifically the last actor which has the lowest `epsilon` exploration.


It is also possible to capture videos of the agent playing the game. To do this, set `capture_video=True` in the `r2d2/run_[gym,, atari].py` scripts. This will record a video of the agent playing the game at given intervals during training. The videos will be saved locally to the log directory. If using wandb, they will also be uploaded to wandb and can be viewed in the wandb dashboard.

## Distributed Architecture

This implementation of R2D2 uses a seperate process for the learner, the replay buffer, and each of the actors. Each actor runs a copy of the environment and continuosly collects trajectory sequences and sends them to the replay, while periodically getting the latest policy parameters from the learner. Meanwhile the replay constantly adds new sequences to the replay buffer as they are sent by the actors, while the learner constantly samples sequences from the replay buffer and updates the policy parameters. 

Each process runs on a single CPU, with the learner also using a GPU if available. This is fine for the environments and models we're using. For larger models you could also assign GPUs to each actor via the `actor_device` parameter, but would have to be careful about running out of GPU memory.

The following diagram shows the overall architecture:

**TODO**
<!---
![R2D2 Distributed Architecture](docs/r2d2/figures/r2d2_architecture.svg)
-->

There are a number of key features to note:

### 1. Batched sequence collection on Actors

Each actor runs a vectorized environment and so collects batches of trajectory sequences with `num_envs_per_actor` sequences collected in parallel using a vectorized environment. This means that each time the actor sends a batch of sequences to the replay buffer, it is actually sending `num_envs_per_actor` sequences. This is more efficient than sending each sequence individually (as they discuss in the original paper).

### 2. Updating the policy network on the Actors

Each actor requests the latest network parameters from the learner periodically (i.e. every `actor_update_interval` steps). This is done by sending a request to the learner process, which then sends back the latest network parameters. 

### 3. Asynchronous parameter sharing on the Learner

The learner process runs a seperate background thread which is responsible for sending the latest policy parameters to the actors. This ensures that there is minimal disruption to the learner process when the actors request the latest parameters. This thread also handles getting the latest metrics from the actors and logging them to Tensorboard (and optionally WandB).

### 4. Shared Memory

Our implementation of R2D2 uses the [torch.multiprocessing](https://pytorch.org/docs/stable/multiprocessing.html) library which handles efficiently sharing data between processes using shared memory. This is greatly reduces the communication overhead between the learner, replay, and actor processes since there is less time spent serializing and deserializing data.

[torch.multiprocessing.queues](https://pytorch.org/docs/stable/multiprocessing.html) are used for communication between processes. This handles moving data into shared memory and efficiently sharing the location of this data between processes.

### 5. Supports single machine only

This implementation of R2D2 is relatively simple, and can lead to big improvements in terms of learning efficiency when running on a single machine with multiple cores. However, it does not support running across multiple machines since it relies on the shared memory of a single machine. This is a reasonable tradeoff for simplicity, but it does mean that we can't scale to hundreds of workers across multiple machines.


## Experiment Results

### Scaling of Steps Per Second (SPS) 

TODO

#### SPS Scaling with Fixed Batch Size


#### SPS Scaling with Fixed Number of Environment per Worker


### Scaling of Learning Speed

### Performance on Atari