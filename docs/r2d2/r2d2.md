# Single Machine R2D2

## Overview

Recurrent Replay Distributed DQN (R2D2) is a variant of the classic off-policy, value-based DQN algorithm with two key features:

1. a recurrent neural network (RNN) for the Q-function
2. a distributed prioritized experience replay buffer, which 

The RNN is trained on sequences of observations, rather than just the usual current observation for most popular DQN agents. The distributed replay buffer is designed to support multiple actors collecting experience in parallel, which can then used by a central learner for updating the Q-function. This combination of RNN based policy plus distributed training allowed R2D2 to achieve SOTA results in Atari and DM-Lab in less wall clock time.

This implementation of R2D2 is designed to be used with a single machine with multiple CPU cores and a single GPU (although GPU is not required, but recommended). Restricting the implementation to a single machine means we can use a relatively simple distributed design and take advantage of using shared memory for communication between processes. It does however limit the scalability of the implementation to the number of cores available on a single machine.

Original Paper:

- [Recurrent Experience Replay in Distributed Reinforcement Learning](https://openreview.net/pdf?id=r1lyTjAqYX)

Further reading and resources:

- [Ape-X paper: Distributed Prioritized Experience Replay](https://arxiv.org/abs/1803.00933) which introduced the distributed replay buffer extended by R2D2
- [Priotizied Experience Replay Paper](https://arxiv.org/pdf/1511.05952.pdf)
- [Deep RL Zoo](https://github.com/michaelnny/deep_rl_zoo) which has a PyTorch implementation of R2D2

## Usage

TODO

## Explanation of logged metrics

TODO

## Distributed Architecture

TODO

## Implementation Details

TODO

## Experiment Results

### Scaling of Steps Per Second (SPS) 

TODO

#### SPS Scaling with Fixed Batch Size


#### SPS Scaling with Fixed Number of Environment per Worker


### Scaling of Learning Speed

### Performance on Atari